# 深層学習day3  
  
##  Section1:再帰型ニューラルネットワークの概念  
  
要点  
・RNNは時系列データ（テキストや音声）に対応可能なNN。  
・誤差逆伝播の際には、BPTT（Back Propagation Through Time）  
を用いて計算する。  
・基本は、これまでと同様、連鎖律を使って計算グラフを辿っていく。  
  
実装演習（3_1_simple_RNN.ipynb）  
・各種パラメータを変更した。  
・各種初期化方を試した。
・活性化関数をReLUに変更した。勾配爆発を確認した。  
・d_tanhを実装し活性化関数をtanhに変更した。収束を確認した。  
  
確認テスト  
・RNNには大きく分けて３つの重みがあり、過去の中間層からの入力を  
受ける重みがある点が、通常の全結合NNと異なる点である。    
  
##  Section2:LSTM   
  
要点  
・RNNの勾配消失、勾配爆発の解決法としていくつかの手法が提案されている。  
・LSTMは、CECといくつかのゲート機構を用いて勾配を上手く伝えることに成功。  
・CECは、記憶機構のみを担当。周りのゲートが記憶への入出力を学習する。  
  
実装演習（predict_word.ipynb）  
・学習のコードを#を外して実行した。  
・エラーが出たので、with tf.variable_scope('auto_reuse', reuse=tf.AUTO_REUSE):  を追加した。  
・some of them looks like fragileという、それらしい単語を予測できた。  
  
確認テスト  
・予測に寄与しない記憶は忘却ゲートが機能して忘れさせる。  
・inputとforgetの値でCECを更新する。  
  
補足  
・ビデオでは「学習結果が終わりました」と話していますが、学習は実行されていません。  
  
##  Section3:GRU  
  
要点  
・LSTMは構造が複雑な為、パラメータ数が多く計算量が多いのが課題。  
・そのパラメータを減らし、計算量を減らす目的でGRUが開発された。   
・GRUにはCECが無く、更新ゲートとリセットゲートとを持っている。  
  
実装演習（predict_word_gru.ipynb）  
・RNNの部分をGRUに変えて実行した。    
・デフォルトの設定では過学習が起き、テストデータの精度が悪かった。     
   
確認テスト  
・LSTMやCECは計算量が多くなる事が課題だった。  
・パラメータを少なくすることでその課題を解決した。  
  
##  Section4:双方向RNN  
  
要点  
・文の推敲や機械翻訳等では、入力文は前からではなく、すべて与えられている。  
・従って、未来⇒過去（文の後ろから前）の流れでも学習する事ができる。  
・両方から学習するRNNを双方向RNNと言い、精度を向上できる場合がある。  
  
実装演習（）
・なし  
  
確認テスト  
・なし  
  
演習チャレンジ  
・双方向RNNでは順方向と逆方向の出力をconcatして全結合層に渡す。  
  
##  Section5:Seq2Seq  
  
要点  
・機械翻訳等に用いられるRNNとしてSeq2Seqというモデルがある。  
・Encoderが入力文をベクトル化して出力し、Decoderがそれを別の文に復元する。  
・HREDは文脈を考慮できるようにEncodingされたベクトルを次の文に渡す。  
・VHREDは潜在変数の概念を用いてHREDの出力にバリエーションを持たせるモデル。  
  
実装演習（）  
・なし  
  
確認テスト  
・Seq2SeqはRNNを用いたEncoder-Decoderモデルの一種。  
・Seq2Seqに文脈を解釈させるのがHRED。HREDの出力を多様にするのがVHRED。  
・VAEはオートエンコーダの潜在変数に確率を導入したもの。    
  
演習チャレンジ  
・単語ごとにembeddingベクターを格納し（行列）最適化する。    
  
##  Section6:Word2Vec  
  
要点  
・Word2Vecは、単語をベクトル表現にする学習方法の１つ。  
・単語数×Embeddingベクトルの次元数の行列で表される。  
・One Hot Encodingを使っていた単語表現に代替する事ができ、  
次元の呪いも回避できる。  
・学習コーパスから学習することができる。  
  
実装演習（）  
・なし  
  
確認テスト  
・なし  
  
##  Section7:Attention Mechanism  
  
要点  
・Seq2Seqは文の長さが長くなった場合に、内部の表現ベクトルの次元数を  
あげる必要がある。（それはAttentionを用いても同じ？）    
・RNNでは、文の長さが長くなった場合に、記憶が保持できなくなる。  
・Attentionは、長い文で単語間の関連性を保持する為に、関連性に重みをつける手法。  
  
実装演習（）  
・なし  
  
確認テスト  
・RNNは時系列データを扱うNNのモデル。W2Vは単語の分散表現を学習するモデル。    
Seq2Seqは時系列データに対応したEncoder Decoderモデル。Attentionは、  
時系列モデルにおいて注目する時刻に重みをつける手法。  
